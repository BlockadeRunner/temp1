# -*- coding: utf-8 -*-
"""Q3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bv-k2fRrbo38G-bnL_9Owu116iS6Pqw7
"""

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Import Statements
# import libraries
import re
import heapq
import numpy as np
import pandas as pd
from urllib.request import urlopen

# if you need to plot anything
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
import matplotlib.pyplot as plt

# Stopword dictionary
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk import download, wordnet
nltk.download('omw-1.4')
download('punkt')
nltk.download('punkt_tab')
download('stopwords')
download("wordnet")

# import gensim
from gensim.parsing.preprocessing import remove_stopwords

# For stemming
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv = CountVectorizer()
tfidf = TfidfVectorizer()

# let's import ML algorithms for classification
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split as tts, KFold, StratifiedKFold, cross_val_score
from sklearn.metrics import accuracy_score as ac, confusion_matrix as cm, ConfusionMatrixDisplay as CMD

# Libraries
import torch
import torch.nn as nn
import torch.optim as optim

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score

# PyTorch NEEDS a data prep or "loader"
from torch.utils.data import DataLoader, TensorDataset

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix

# Step 1: Import the wine dataset
# Instructions: "For this question, we will use the winemagdata130kv2 dataset..."
wine_data = pd.read_csv('https://github.com/dvasiliu/AML/blob/main/Data%20Sets/winemagdata130kv2.csv?raw=true',quoting=2)
wines = wine_data[["description","points"]]

# Step 2: Subset the dataset
# Instructions: "...we randomly subset 10000 reviews with a random state = 1693."
wines_subset = wines.sample(10000,random_state=1693).reset_index(drop=True)

# Step 3: Preprocess the reviews
# Instructions: "We preprocess the customer reviews in the sense of Natural Language Processing (with all the pre-processing steps explained in class, ..."
def text_preprocess(original_documents):
  reviews = original_documents.description.values + ' '+'fakeword'+' '
  joined_reviews = ' '
  for i in range(len(reviews)):
    joined_reviews = joined_reviews+reviews[i]
  # here we do the text pre-processing very fast
  # remove punctuation
  descriptions = re.sub('[^a-zA-Z0-9 ]','',joined_reviews)
  # remove stopwords
  descriptions = remove_stopwords(descriptions.lower())
  # we can use Porter Stemmer or we can Lemmatize
  # for Porter Stemmer
  descriptions = [stemmer.stem(word) for word in descriptions.split()]
  # next we can stem or lemmatize
  #descriptions = [lemma.lemmatize(word) for word in descriptions.split()]
  descriptions = " ".join(descriptions)
  documents = descriptions.split('fakeword')
  documents = documents[:-1]
  return documents

documents = text_preprocess(wines_subset)

# Step 4: TF-IDF transform
# Instructions: "...and by using the TF-IDF transform)."
vectorizer = TfidfVectorizer()
x = vectorizer.fit_transform(documents)

# Step 5: Label the wines
# Instructions: "The wines with at least 92 points (in the target variable) are labeled Excellent, those between 87 and 91 are Good, and those below 87 are OK."
pts = wines_subset.points
y = pts.copy().values
y[pts>=92] = 2                    #  Excellent (2)
y[(pts>=87) & (pts<=91)] = 1      #  Good (1)
y[(pts<87)] = 0                   #  OK (0)

# Step 6: Create Train, Test splits
# Instructions: "We subset the data into train and test sets, (with the random state remaining 1693) such that the Test set is 25% of the total size, ..."
X_train, X_test, Y_train, Y_test = tts(x,y,test_size=0.25,random_state=1693)

# Step 7: Pytorch Neural Network
# Instructions: "...and we train a PyTorch neural network classifier that has 300 neurons in the first layer and 200 neurons in the second layer,
#                both layers having Parametric Rectified Linear Unit activations with the highest number of channels."
class WineNet(nn.Module):
    def __init__(self, n_features):
        super(WineNet, self).__init__()
        self.fc1 = nn.Linear(n_features, 300).float()  # "300 neurons in the first layer"
        self.fc2 = nn.Linear(300, 200).float()  # "200 neurons in the second layer"
        self.fc3 = nn.Linear(200, 3).float()  # 3 neurons in output layer for "Excellent (2)", "Good (1)", and "OK (0)"

        # Parametric ReLU activations
        self.prelu1 = nn.PReLU()
        self.prelu2 = nn.PReLU()

    def forward(self, x):
        # Apply PReLU activations
        x = self.prelu1(self.fc1(x))
        x = self.prelu2(self.fc2(x))
        x = self.fc3(x)
        return x

# Step 8: Training and Evaluating
# Instructions: "Train the network using a batch size of 25 and 50 epochs and the Adaptive momentum gradient descent with weight decay and a learning rate of 0.001."
# Instructions: "When this model is applied to the test set, we see that the number of Good instances correctly classified as Good is close to: "
def train_and_evaluate(x_train, y_train, x_test, y_test, num_epochs=50, batch_size=25):
  # Standardize data
  # scaler = StandardScaler()
  # X_train_scaled = scaler.fit_transform(x_train)
  # X_test_scaled = scaler.transform(x_test)
  x_train = x_train.toarray()
  x_test = x_test.toarray()



  # Convert the data to PyTorch tensors
  X_train_tensor = torch.tensor(x_train, dtype=torch.float32)
  X_test_tensor = torch.tensor(x_test, dtype=torch.float32)
  y_train_tensor = torch.tensor(y_train, dtype=torch.long)
  y_test_tensor = torch.tensor(y_test, dtype=torch.long)

  # Create the data loader
  train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
  test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Inherits batch size of 25 as listed in function parameters
  test_loader = DataLoader(test_dataset, shuffle=False)

  # Initialize the model, loss function, and optimizer
  model = WineNet(x_train.shape[1])   # here we pass in the number of features
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.AdamW(model.parameters(), lr=0.001)  # Adam optimizer with specified lr and weight decay

  # Train the model
  for epoch in range(num_epochs):
      model.train()
      for batch in train_loader:
          X_batch, y_batch = batch
          optimizer.zero_grad()
          outputs = model(X_batch)
          loss = criterion(outputs, y_batch)
          loss.backward()
          optimizer.step()

      # # Evaluate after each epoch
      # model.eval()
      # with torch.no_grad():
      #     correct = 0
      #     total = 0
      #     for batch in test_loader:
      #         X_batch, y_batch = batch
      #         outputs = model(X_batch)
      #         _, predicted = torch.max(outputs.data, 1)
      #         total += y_batch.size(0)
      #         correct += (predicted == y_batch).sum().item()
      #     print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {100 * correct / total:.2f}%')

  # Evaluation on the "Good" wines
  model.eval()
  with torch.no_grad():
      good_correct = 0
      good_total = 0
      for batch in test_loader:
          X_batch, y_batch = batch
          outputs = model(X_batch)
          _, predicted = torch.max(outputs.data, 1)
          good_correct += ((predicted == y_batch) & (y_batch == 1)).sum().item()  # Correctly classified "Good" wines
          good_total += (y_batch == 1).sum().item()

      # print(f'Good wines correctly classified as Good: {good_correct}/{good_total} = {good_correct / good_total:.2f}')
      print(f'Number of Good instances correctly classified as Good is close to: {good_correct:.2f}')



train_and_evaluate(X_train, Y_train, X_test, Y_test)

